# =============================================================================
# API Authentication
# =============================================================================

# API key(s) for accessing all endpoints (REQUIRED for security!)
# Supports multiple comma-separated keys: key1,key2,key3
API_KEY=your-secret-api-key-here

# Admin API key(s) for privileged operations (add_lora, etc.)
# Supports multiple comma-separated keys: adminkey1,adminkey2
ADMIN_API_KEY=your-admin-api-key-here

# Enable/disable authentication (set to False only for development!)
REQUIRE_AUTH=True

# =============================================================================
# MongoDB Configuration (Optional)
# =============================================================================

# Use MongoDB for authentication and bot settings storage
# If disabled, will use environment variables for authentication
USE_MONGODB=false

# MongoDB connection URL (for Docker: mongodb://admin:changeme@mongodb:27017/)
MONGODB_URL=mongodb://localhost:27017/

# MongoDB database name
MONGODB_DB=ai_apis

# MongoDB root credentials (for Docker setup)
MONGO_ROOT_USER=admin
MONGO_ROOT_PASSWORD=changeme

# Admin Telegram user for bot initialization
# Set this to automatically add an admin user during MongoDB setup
# Format: user_id (numeric Telegram user ID)
ADMIN_TELEGRAM_USER_ID=
ADMIN_TELEGRAM_USERNAME=

# =============================================================================
# External Service Tokens
# =============================================================================

# HuggingFace API token (for model downloads)
HF_TOKEN=hf_your_huggingface_token_here

# Civitai API key (for LORA model downloads)
CIVIT_KEY=your_civit_api_key_here

# Telegram Bot API token
TELEGRAM_TOKEN=1234567890:ABCdefGHIjklMNOpqrsTUVwxyz123456789

# =============================================================================
# API Endpoints
# =============================================================================
# Configure these to point to your AI service endpoints
# For Docker: use service names (e.g., stable_diffusion, whisper, ollama)
# For local development: use localhost
# For production: use your server IPs/hostnames

# OLLAMA (Primary LLM)
OLLAMA_HOST=localhost
OLLAMA_PORT=2345
OLLAMA_MODEL=llama3.3
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=2000

# Stable Diffusion API
SD_HOST=localhost
SD_PORT=8000

# Whisper Audio Transcription
WHISPER_HOST=localhost
WHISPER_PORT=8080

# Legacy LLM Endpoints (for backward compatibility)
LLM_MIXTRAL_HOST=localhost
LLM_MIXTRAL_PORT=8000

LLM_COMMAND_R_HOST=localhost
LLM_COMMAND_R_PORT=1234

# =============================================================================
# Model Settings
# =============================================================================

# Default Whisper model (tiny, base, small, medium, large, turbo)
DEFAULT_WHISPER_MODEL=turbo

# Default Stable Diffusion model
DEFAULT_SD_MODEL=stabilityai/stable-diffusion-2-1

# Default torch dtype (float16, float32, bfloat16)
DEFAULT_TORCH_DTYPE=float16

# =============================================================================
# Server Settings
# =============================================================================

# Request timeout in seconds
REQUEST_TIMEOUT=300

# Max upload size in bytes (100MB default)
MAX_UPLOAD_SIZE=104857600
